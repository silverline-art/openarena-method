# EXMO Gait Analysis Pipeline - Comprehensive Architecture Review
**Date:** 2025-11-21  
**Version Analyzed:** 1.0.0  
**Reviewer:** System Architect AI  
**Total Codebase:** ~4,926 lines of Python

---

## Executive Summary

The EXMO gait analysis pipeline is a **monolithic procedural architecture** with clear layer separation but lacks modern design patterns for extensibility. The system demonstrates **solid fundamentals** with well-organized modules, but **production-readiness is compromised** by tight coupling, limited abstraction, and absence of dependency injection.

**Key Findings:**
- âœ… **Strengths:** Clear separation of concerns, comprehensive domain coverage, YAML configuration
- âš ï¸ **Weaknesses:** Tight coupling, no plugin architecture, limited testability, hardcoded dependencies
- ðŸ”´ **Critical Issues:** God object in cli.py, no interface abstractions, version fragmentation (v1.1, v1.2)
- ðŸ“Š **Scalability Rating:** 5/10 (suitable for research prototype, not production-grade)

---

## 1. Architecture Overview

### 1.1 High-Level Architecture Diagram (Text-Based)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER INTERFACE LAYER                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  cli.py (316 LOC)                    batch_process.py (524 LOC)     â”‚
â”‚  - Argument parsing                  - Parallel processing          â”‚
â”‚  - Pipeline orchestration            - Progress tracking            â”‚
â”‚  - Logging setup                     - Summary reporting            â”‚
â”‚  âŒ GOD OBJECT ANTIPATTERN            - ProcessPoolExecutor          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CORE PROCESSING PIPELINE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  DATA LOADING    â”‚ â†’  â”‚  PREPROCESSING   â”‚ â†’  â”‚   ANALYSIS   â”‚  â”‚
â”‚  â”‚                  â”‚    â”‚                  â”‚    â”‚              â”‚  â”‚
â”‚  â”‚ MultiViewData    â”‚    â”‚ DataPreprocessor â”‚    â”‚ PhaseDetectorâ”‚  â”‚
â”‚  â”‚ Loader           â”‚    â”‚ - Smoothing      â”‚    â”‚ StepDetector â”‚  â”‚
â”‚  â”‚ (220 LOC)        â”‚    â”‚ - Scaling        â”‚    â”‚ MetricsComp  â”‚  â”‚
â”‚  â”‚                  â”‚    â”‚ - CoM calc       â”‚    â”‚ (694 LOC)    â”‚  â”‚
â”‚  â”‚ - CSV parsing    â”‚    â”‚ (278 LOC)        â”‚    â”‚              â”‚  â”‚
â”‚  â”‚ - 3-view sync    â”‚    â”‚                  â”‚    â”‚ ROM + Gait   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                          â†“           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  STATISTICS      â”‚ â†  â”‚  EXPORT          â”‚ â†  â”‚              â”‚  â”‚
â”‚  â”‚                  â”‚    â”‚                  â”‚    â”‚              â”‚  â”‚
â”‚  â”‚ Aggregator       â”‚    â”‚ XLSXExporter     â”‚    â”‚              â”‚  â”‚
â”‚  â”‚ (276 LOC)        â”‚    â”‚ Visualizer       â”‚    â”‚              â”‚  â”‚
â”‚  â”‚                  â”‚    â”‚ (453 + 596 LOC)  â”‚    â”‚              â”‚  â”‚
â”‚  â”‚ - Summary stats  â”‚    â”‚                  â”‚    â”‚              â”‚  â”‚
â”‚  â”‚ - Median/MAD     â”‚    â”‚ - Excel sheets   â”‚    â”‚              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ - PNG dashboards â”‚    â”‚              â”‚  â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      UTILITY SERVICES LAYER                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  signal_processing.py (267 LOC)    geometry.py (335 LOC)            â”‚
â”‚  - Savitzky-Golay filter            - Angle computation             â”‚
â”‚  - MAD computation                  - Scaling factors               â”‚
â”‚  - Peak detection                   - CoM calculation               â”‚
â”‚  - Interpolation                    - Symmetry index                â”‚
â”‚                                                                       â”‚
â”‚  validation.py (198 LOC)            style.py (433 LOC)              â”‚
â”‚  - Frame rate checks                - Matplotlib theming            â”‚
â”‚  - Keypoint validation              - Publication-grade plots       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CONFIGURATION LAYER                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  config.yaml (218 lines)            config_v1.2_calibrated.yaml     â”‚
â”‚  - Global settings (FPS, thresholds)                                â”‚
â”‚  - Experiment groups (control, low/med/high dose)                   â”‚
â”‚  - File patterns, output structure                                  â”‚
â”‚  âš ï¸ NO SCHEMA VALIDATION             âš ï¸ VERSION FRAGMENTATION        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Data Flow Pipeline

```
INPUT (CSV files)
   â”‚
   â”œâ”€ TOP view    â”€â”€â”
   â”œâ”€ SIDE view   â”€â”€â”¤â”€â†’ MultiViewDataLoader
   â””â”€ BOTTOM view â”€â”€â”˜       â”‚
                            â”‚ (Synchronize, extract keypoints)
                            â†“
                    KEYPOINT TRAJECTORIES
                            â”‚
                            â†“
                    DataPreprocessor
                    â”‚ 1. Filter outliers (MAD)
                    â”‚ 2. Interpolate gaps
                    â”‚ 3. Smooth (Savgol)
                    â”‚ 4. Compute scale factor
                    â”‚ 5. Convert to cm
                            â†“
                    PREPROCESSED DATA
                    â”œâ”€ CoM trajectory
                    â”œâ”€ Paw trajectories (Ã—4)
                    â””â”€ Joint keypoints
                            â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                      â”‚
  PhaseDetector                         StepDetector
  â”‚ - CoM speed                         â”‚ - Vertical position peaks
  â”‚ - MAD thresholds                    â”‚ - Velocity minima
  â”‚ - Walking windows                   â”‚ - Swing/stance phases
  â”‚ - Stationary windows                â”‚ - Foot strikes
        â”‚                                      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                    GaitMetricsComputer
                    â”‚ - Cadence, duty cycle
                    â”‚ - Stride length/time
                    â”‚ - Regularity index
                    â”‚ - Coordination
                            â†“
                    ROMMetricsComputer
                    â”‚ - Joint angles (3-point)
                    â”‚ - Angular velocity
                    â”‚ - CoM sway (ML/AP)
                    â”‚ - Asymmetry index
                            â†“
                    StatisticsAggregator
                    â”‚ - Median, SD, MAD
                    â”‚ - Corrected mean (v1.2)
                    â”‚ - Confidence intervals
                            â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                      â”‚
  XLSXExporter                        DashboardVisualizer
  â”‚ - 5 sheets (Summary,              â”‚ - Coordination plots
  â”‚   Gait, ROM, Events,              â”‚ - Speed/spatial
  â”‚   Metadata)                       â”‚ - ROM visualization
  â”‚ - Intermediate CSVs               â”‚ - Publication-ready
        â”‚                                      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                    OUTPUT FILES
                    â”œâ”€ Gait_Analysis_*.xlsx
                    â”œâ”€ plot_*.png (Ã—4)
                    â””â”€ intermediates/*.csv
```

---

## 2. Directory Structure Analysis

### 2.1 Current Organization

```
Exmo-Open/
â”œâ”€â”€ src/exmo_gait/              # Main package (4,926 LOC)
â”‚   â”œâ”€â”€ __init__.py             # Empty (2 LOC)
â”‚   â”œâ”€â”€ cli.py                  # 316 LOC âŒ GOD OBJECT
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                   # Data loading & preprocessing
â”‚   â”‚   â”œâ”€â”€ data_loader.py      # 220 LOC âœ… Well-scoped
â”‚   â”‚   â””â”€â”€ preprocessor.py     # 278 LOC âœ… Single responsibility
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis/               # Gait & ROM analysis
â”‚   â”‚   â”œâ”€â”€ phase_detector.py   # 294 LOC âœ… Clean API
â”‚   â”‚   â”œâ”€â”€ step_detector.py    # 318 LOC âœ… Focused
â”‚   â”‚   â””â”€â”€ metrics_computer.py # 694 LOC âš ï¸ Large, 2 classes
â”‚   â”‚
â”‚   â”œâ”€â”€ statistics/             # Aggregation
â”‚   â”‚   â””â”€â”€ aggregator.py       # 276 LOC âœ… Good encapsulation
â”‚   â”‚
â”‚   â”œâ”€â”€ export/                 # Visualization & export
â”‚   â”‚   â”œâ”€â”€ xlsx_exporter.py    # 246 LOC âœ… Clean
â”‚   â”‚   â”œâ”€â”€ visualizer.py       # 453 LOC âš ï¸ Medium complexity
â”‚   â”‚   â”œâ”€â”€ visualizer_enhanced.py # 596 LOC âš ï¸ Duplication
â”‚   â”‚   â””â”€â”€ style.py            # 433 LOC âœ… Good separation
â”‚   â”‚
â”‚   â””â”€â”€ utils/                  # Shared utilities
â”‚       â”œâ”€â”€ signal_processing.py # 267 LOC âœ… Pure functions
â”‚       â”œâ”€â”€ geometry.py          # 335 LOC âœ… Mathematical ops
â”‚       â””â”€â”€ validation.py        # 198 LOC âœ… Validation logic
â”‚
â”œâ”€â”€ batch_process.py            # 524 LOC âš ï¸ Feature-rich but fragile
â”œâ”€â”€ config.yaml                 # 218 lines âš ï¸ No schema validation
â”œâ”€â”€ config_v1.2_calibrated.yaml # âš ï¸ Version fragmentation
â”œâ”€â”€ main.py                     # Wrapper script
â””â”€â”€ tests/                      # âŒ EMPTY - NO TESTS
```

### 2.2 Separation of Concerns Assessment

| Layer | Responsibility | Implementation Quality | Issues |
|-------|---------------|----------------------|--------|
| **CLI** | User interface, orchestration | âš ï¸ Moderate | God object, hardcoded pipeline |
| **Core** | Data loading, preprocessing | âœ… Good | Well-scoped, clear APIs |
| **Analysis** | Phase/step detection, metrics | âœ… Good | Could extract interfaces |
| **Statistics** | Aggregation, summary stats | âœ… Good | Clean, reusable |
| **Export** | XLSX, PNG visualization | âš ï¸ Moderate | Duplication (2 visualizers) |
| **Utils** | Signal processing, geometry | âœ… Excellent | Pure functions, testable |

**Verdict:** âœ… **Good separation** at module level, âš ï¸ **Poor abstraction** at class level.

---

## 3. Module Dependency Analysis

### 3.1 Import Dependency Graph

```
cli.py (ORCHESTRATOR)
 â”œâ”€â–º core.data_loader (MultiViewDataLoader)
 â”œâ”€â–º core.preprocessor (DataPreprocessor)
 â”œâ”€â–º analysis.phase_detector (PhaseDetector)
 â”œâ”€â–º analysis.step_detector (StepDetector)
 â”œâ”€â–º analysis.metrics_computer (GaitMetricsComputer, ROMMetricsComputer)
 â”œâ”€â–º statistics.aggregator (StatisticsAggregator)
 â”œâ”€â–º export.xlsx_exporter (XLSXExporter)
 â”œâ”€â–º export.visualizer (DashboardVisualizer)
 â””â”€â–º export.visualizer_enhanced (EnhancedDashboardVisualizer)

core.preprocessor
 â”œâ”€â–º utils.signal_processing (apply_savgol_filter, interpolate_missing_values, filter_outliers_mad)
 â”œâ”€â–º utils.geometry (compute_scaling_factor, pixels_to_cm, compute_center_of_mass)
 â””â”€â–º utils.validation (validate_scaling_factor)

core.data_loader
 â””â”€â–º utils.validation (validate_frame_rate, validate_keypoints_present)

analysis.phase_detector
 â”œâ”€â–º utils.signal_processing (compute_mad, smooth_binary_classification, compute_velocity)
 â””â”€â–º utils.geometry (compute_trajectory_speed)

analysis.step_detector
 â”œâ”€â–º scipy.signal (find_peaks)
 â”œâ”€â–º utils.signal_processing (compute_mad, detect_peaks_adaptive)
 â””â”€â–º utils.validation (validate_sufficient_strides)

analysis.metrics_computer
 â”œâ”€â–º utils.geometry (compute_stride_length, compute_trajectory_speed, compute_angle_3points, etc.)
 â””â”€â–º utils.signal_processing (compute_mad)

statistics.aggregator
 â”œâ”€â–º utils.signal_processing (compute_mad)
 â””â”€â–º scipy.stats (trim_mean) [v1.2.0 only]

export.visualizer_enhanced
 â””â”€â–º export.style (COLORS, FONTS, configure_style, format_axis)

batch_process.py
 â””â”€â–º cli.run_pipeline (TIGHT COUPLING)
```

### 3.2 Circular Dependency Check

**Result:** âœ… **NO circular dependencies detected**

All imports follow a strict layered architecture:
```
CLI â†’ Analysis/Export â†’ Core â†’ Utils
```

### 3.3 Coupling Analysis

#### Tight Coupling Issues

1. **cli.py â†’ All modules** (10 direct imports)
   - **Problem:** Any change in analysis modules breaks CLI
   - **Solution:** Dependency injection, plugin architecture

2. **batch_process.py â†’ cli.run_pipeline**
   - **Problem:** Hardcoded call to specific function
   - **Solution:** Abstract interface for pipeline execution

3. **Hardcoded FPS value (120.0)** appears in:
   - `cli.py:75`, `cli.py:134`, `cli.py:149`, `cli.py:165`, `cli.py:175`, `cli.py:202`
   - `xlsx_exporter.py:141`, `xlsx_exporter.py:166`
   - **Problem:** Magic number scattered across codebase
   - **Solution:** Global constant or config injection

4. **Version fragmentation** (v1.1 vs v1.2 methods):
   - `compute_scale_factor()` vs `compute_scale_factor_v2()`
   - `compute_summary_stats()` vs `compute_summary_stats_v2()`
   - **Problem:** Technical debt, unclear deprecation path
   - **Solution:** Feature flags or migration strategy

#### Loose Coupling (Good Examples)

âœ… **utils/** modules are pure functions with no internal dependencies  
âœ… **StatisticsAggregator** operates on generic dictionaries  
âœ… **Signal processing** functions are reusable across modules

### 3.4 God Objects & Antipatterns

#### ðŸ”´ Critical: `cli.py::run_pipeline()` (316 LOC function)

**Antipattern:** God Function with 10 responsibilities:
1. Logging setup
2. Data loading
3. Keypoint extraction
4. Preprocessing
5. CoM computation
6. Phase detection
7. Step detection
8. Gait metrics
9. ROM metrics
10. Export + visualization

**Evidence:**
```python
def run_pipeline(top_path, side_path, bottom_path, output_dir, verbose, config):
    # Lines 41-273: Single 232-line function doing EVERYTHING
    logger = setup_logging(...)          # Step 1/10
    loader = MultiViewDataLoader(...)    # Step 2/10
    preprocessor = DataPreprocessor(...) # Step 3/10
    phase_detector = PhaseDetector(...)  # Step 5/10
    step_detector = StepDetector(...)    # Step 6/10
    gait_computer = GaitMetricsComputer(...)  # Step 7/10
    rom_computer = ROMMetricsComputer(...)    # Step 8/10
    aggregator = StatisticsAggregator(...)    # Step 9/10
    exporter = XLSXExporter(...)              # Step 10/10
    visualizer = DashboardVisualizer(...)     # Step 10/10
```

**Impact:**
- âŒ Impossible to test individual pipeline stages
- âŒ Cannot swap components (e.g., different metrics)
- âŒ Cannot parallelize pipeline stages
- âŒ No checkpoint/resume capability

---

## 4. Design Patterns Analysis

### 4.1 Current Patterns (Limited)

| Pattern | Location | Assessment |
|---------|----------|------------|
| **Procedural Pipeline** | `cli.run_pipeline()` | âš ï¸ Works but rigid |
| **Strategy (Partial)** | `DashboardVisualizer` vs `EnhancedDashboardVisualizer` | âš ï¸ No interface abstraction |
| **Facade** | `batch_process.py` | âš ï¸ Hardcoded to `cli.run_pipeline` |
| **Pure Functions** | `utils/` modules | âœ… Excellent for testability |

### 4.2 Missing Patterns (Critical Gaps)

#### âŒ 1. Factory Pattern (Component Creation)
**Problem:** Direct instantiation everywhere
```python
# Current: Hardcoded in cli.py
preprocessor = DataPreprocessor(
    smoothing_window=gs.get('smoothing_window', 11),
    smoothing_poly=gs.get('smoothing_poly', 3),
    ...
)
```

**Needed:**
```python
# Proposed: Factory for flexible creation
class PreprocessorFactory:
    @staticmethod
    def create_from_config(config: Dict) -> IPreprocessor:
        return DataPreprocessor(**config['preprocessing'])
```

#### âŒ 2. Strategy Pattern (Algorithm Selection)
**Problem:** `if/else` for visualizer selection
```python
# Current: cli.py:226-240
if use_enhanced:
    visualizer = EnhancedDashboardVisualizer(...)
else:
    visualizer = DashboardVisualizer(...)
```

**Needed:**
```python
# Proposed: Strategy interface
class IVisualizer(ABC):
    @abstractmethod
    def generate_all_dashboards(self, ...) -> List[Path]: ...

class VisualizerFactory:
    @staticmethod
    def create(style: str, config: Dict) -> IVisualizer:
        return {
            'standard': DashboardVisualizer,
            'enhanced': EnhancedDashboardVisualizer
        }[style](config)
```

#### âŒ 3. Observer Pattern (Progress Tracking)
**Problem:** No callback mechanism for long-running operations
```python
# Current: Silent processing
step_results = step_detector.detect_all_limbs(paw_trajectories, walking_windows)
```

**Needed:**
```python
# Proposed: Progress callbacks
class ProgressObserver(ABC):
    @abstractmethod
    def on_progress(self, stage: str, percent: float, message: str): ...

step_detector.add_observer(ProgressLogger())
step_detector.detect_all_limbs(...)  # Fires progress events
```

#### âŒ 4. Chain of Responsibility (Pipeline Stages)
**Problem:** Monolithic pipeline, no stage abstraction
```python
# Proposed: Stage-based pipeline
class PipelineStage(ABC):
    @abstractmethod
    def execute(self, context: PipelineContext) -> PipelineContext: ...

pipeline = Pipeline([
    LoadDataStage(),
    PreprocessStage(),
    AnalyzeStage(),
    ExportStage()
])
result = pipeline.run(config)
```

---

## 5. Configuration System Analysis

### 5.1 Current System (YAML-based)

**Strengths:**
âœ… Comprehensive coverage (218 lines in `config.yaml`)  
âœ… Hierarchical organization (global_settings, experiment_groups, etc.)  
âœ… Sample-specific overrides supported  
âœ… Human-readable and version-controllable

**Weaknesses:**
âŒ No schema validation (accepts invalid configs silently)  
âŒ Version fragmentation (`config.yaml` vs `config_v1.2_calibrated.yaml`)  
âŒ No environment-based configs (dev/staging/prod)  
âŒ Hardcoded defaults scattered in code instead of config  
âŒ No config versioning/migration system

### 5.2 Configuration Loading Flow

```
config.yaml (YAML file)
     â†“
batch_process.py::load_config()
     â†“ yaml.safe_load()
     â†“ Manual validation (required_fields check)
     â†“
cli.run_pipeline(config=config)
     â†“ Extract global_settings
     â†“
gs = config['global_settings']  # Used inline throughout
```

**Problems:**
1. **No type safety** - `gs.get('smoothing_window', 11)` assumes int but no enforcement
2. **Scattered defaults** - Default values in `.get()` calls across 15+ files
3. **Version hell** - `config_v1.2_calibrated.yaml` has different schema than `config.yaml`

### 5.3 Recommended Configuration Architecture

```python
# config/schema.py
from pydantic import BaseModel, Field, validator

class PreprocessingConfig(BaseModel):
    smoothing_window: int = Field(11, ge=3, le=21, description="Savgol window")
    smoothing_poly: int = Field(3, ge=1, le=5)
    outlier_threshold: float = Field(3.0, ge=1.0, le=10.0)
    
    @validator('smoothing_window')
    def window_must_be_odd(cls, v):
        if v % 2 == 0:
            raise ValueError('smoothing_window must be odd')
        return v

class GlobalConfig(BaseModel):
    fps: float = 120.0
    preprocessing: PreprocessingConfig = PreprocessingConfig()
    phase_detection: PhaseDetectionConfig = PhaseDetectionConfig()
    # ... etc

# config/loader.py
def load_config(path: Path) -> GlobalConfig:
    with open(path) as f:
        raw = yaml.safe_load(f)
    return GlobalConfig(**raw)  # Pydantic validation
```

**Benefits:**
âœ… Type safety with IDE autocomplete  
âœ… Automatic validation with clear error messages  
âœ… Self-documenting schema (Field descriptions)  
âœ… Easy config versioning (Pydantic migrations)

---

## 6. CLI Architecture Analysis

### 6.1 Current CLI Design

**File:** `cli.py` (316 LOC)

**Components:**
1. `setup_logging()` - Configures file + console logging
2. `run_pipeline()` - **God function** (232 LOC)
3. `main()` - Argument parsing + entry point

**Strengths:**
âœ… Comprehensive argument parsing (argparse)  
âœ… Logging setup with file + console handlers  
âœ… JSON output for programmatic use  
âœ… Exit codes (0 = success, 2 = error)

**Weaknesses:**
âŒ No subcommands (analyze, export, validate, etc.)  
âŒ No dry-run mode (besides batch_process.py)  
âŒ No config generation helpers  
âŒ No intermediate checkpoint/resume  
âŒ Pipeline hardcoded (no plugin loading)

### 6.2 Argument Parsing Assessment

```python
# Current: Simple but limited
parser.add_argument('--top', type=Path, required=True)
parser.add_argument('--side', type=Path, required=True)
parser.add_argument('--bottom', type=Path, required=True)
parser.add_argument('--output', type=Path, required=True)
parser.add_argument('--verbose', '-v', action='store_true')
```

**Problems:**
1. No config file argument (uses hardcoded config loading in batch_process.py)
2. No override flags (e.g., `--fps`, `--smoothing-window`)
3. No validation of input file existence before pipeline start
4. No auto-discovery of input files (must specify all 3 views)

### 6.3 Error Handling Strategy

**Current:**
```python
try:
    # 232 lines of processing
    return {'status': 'success', ...}
except Exception as e:
    logger.error(f"Pipeline failed: {str(e)}", exc_info=True)
    return {'status': 'error', 'error': str(e)}
```

**Issues:**
âŒ Single catch-all exception handler  
âŒ No retry logic for transient failures  
âŒ No partial result recovery (all-or-nothing)  
âŒ Stack traces in logs but not structured for debugging

**Recommended:**
```python
class PipelineError(Exception): pass
class DataLoadError(PipelineError): pass
class PreprocessingError(PipelineError): pass
class AnalysisError(PipelineError): pass

try:
    # Stage-by-stage with specific exceptions
except DataLoadError as e:
    # Recoverable: suggest file fixes
except PreprocessingError as e:
    # Recoverable: save intermediate data
except AnalysisError as e:
    # Partial results available
```

---

## 7. Critical Architecture Issues

### 7.1 Testability

**Test Coverage:** âŒ **0%** (tests/ directory is empty)

**Why Untestable:**
1. God function `run_pipeline()` - too large to unit test
2. Hardcoded dependencies - no mocking possible
3. File I/O mixed with logic - requires filesystem setup
4. No dependency injection - cannot substitute test doubles

**Impact:** ðŸ”´ **CRITICAL**
- No regression testing
- Refactoring is extremely risky
- Bug fixes may introduce new bugs
- Cannot verify correctness programmatically

### 7.2 Extensibility

**How to add new metric?**
Current process:
1. Modify `metrics_computer.py::compute_all_gait_metrics()`
2. Modify `aggregator.py::aggregate_gait_metrics()`
3. Modify `xlsx_exporter.py::create_gait_metrics_sheet()`
4. Modify `visualizer.py` if visualization needed
5. Update config schema

**Problems:**
âŒ Requires editing 4-5 files (violates Open/Closed Principle)  
âŒ No plugin system for custom metrics  
âŒ No registration mechanism

**Recommended:**
```python
# Proposed: Metric plugin system
class IGaitMetric(ABC):
    @abstractmethod
    def compute(self, step_results, trajectories) -> Dict: ...
    
    @property
    @abstractmethod
    def name(self) -> str: ...

class MetricRegistry:
    _metrics: Dict[str, Type[IGaitMetric]] = {}
    
    @classmethod
    def register(cls, metric_class: Type[IGaitMetric]):
        cls._metrics[metric_class.name] = metric_class
    
    @classmethod
    def compute_all(cls, data) -> Dict:
        return {
            name: metric_class().compute(data)
            for name, metric_class in cls._metrics.items()
        }

# Usage:
@MetricRegistry.register
class CustomCadenceMetric(IGaitMetric):
    name = "custom_cadence"
    def compute(self, data): ...
```

### 7.3 Data Format Flexibility

**Current:** Hardcoded CSV parsing (DLC format)

```python
# data_loader.py:44
df = pd.read_csv(filepath, header=[0, 1, 2])
scorer = df.columns[1][0]
bodyparts = df.columns.get_level_values(1)
```

**Problems:**
âŒ Only supports DeepLabCut CSV format  
âŒ Cannot handle SLEAP, OpenPose, or other formats  
âŒ No adapter pattern for format conversion

**Recommended:**
```python
class IDataLoader(ABC):
    @abstractmethod
    def load(self, filepath: Path) -> KeypointData: ...

class DLCDataLoader(IDataLoader):
    def load(self, filepath: Path):
        # Current CSV parsing logic
        
class SLEAPDataLoader(IDataLoader):
    def load(self, filepath: Path):
        # SLEAP H5 parsing

class DataLoaderFactory:
    @staticmethod
    def create(format: str) -> IDataLoader:
        return {
            'dlc': DLCDataLoader,
            'sleap': SLEAPDataLoader,
            'openpose': OpenPoseDataLoader
        }[format]()
```

### 7.4 Version Fragmentation

**Evidence:**
- `compute_scale_factor()` (v1.1) vs `compute_scale_factor_v2()` (v1.2)
- `compute_summary_stats()` vs `compute_summary_stats_v2()`
- `config.yaml` vs `config_v1.2_calibrated.yaml`

**Problems:**
âŒ No clear deprecation policy  
âŒ Both versions maintained simultaneously  
âŒ Unclear which version to use  
âŒ Technical debt accumulation

**Recommended:**
```python
# Use feature flags instead of versioned methods
class DataPreprocessor:
    def __init__(self, use_v2_scaling: bool = True):
        self.use_v2_scaling = use_v2_scaling
    
    def compute_scale_factor(self, ...):
        if self.use_v2_scaling:
            return self._compute_scale_factor_v2(...)
        else:
            warnings.warn("v1.1 scaling is deprecated", DeprecationWarning)
            return self._compute_scale_factor_v1(...)
```

---

## 8. Scalability Assessment

### 8.1 Performance Bottlenecks

**Identified bottlenecks:**

1. **Sequential pipeline** (no parallelization within single sample)
   ```python
   # cli.py lines 74-180 run sequentially
   loader.load_all_views(...)        # ~200ms
   preprocessor.batch_preprocess(...)  # ~1.5s
   phase_detector.detect_walking(...)  # ~500ms
   step_detector.detect_all_limbs(...) # ~800ms
   gait_computer.compute_all(...)      # ~300ms
   ```
   **Total:** ~3.3s per sample (could be ~1s with parallelization)

2. **Matplotlib rendering** (visualizer.py)
   - 4 separate plot generations (not parallelized)
   - High-DPI rendering (300 DPI default)
   - **Estimated:** ~2-3s per sample

3. **Excel export with pandas**
   - 5 sheets written sequentially
   - No streaming (loads all data in memory)
   - **Estimated:** ~500ms per sample

**Total per-sample time:** ~6-7 seconds

### 8.2 Memory Footprint

**Current memory usage (estimated):**
- Raw data (3 CSV files, ~5000 frames Ã— 20 keypoints): ~6 MB
- Preprocessed trajectories: ~8 MB
- Metrics dictionaries: ~500 KB
- Matplotlib figures (4 Ã— 300 DPI): ~20 MB
- **Peak memory:** ~35 MB per sample

**Scalability:**
âœ… Single sample: No issues  
âœ… Batch processing (parallel): 35 MB Ã— 4 workers = 140 MB (acceptable)  
âš ï¸ Large studies (>100 samples): Memory OK, but no streaming

### 8.3 Horizontal Scaling

**Current:** `batch_process.py` with `ProcessPoolExecutor`

```python
# batch_process.py:462
with ProcessPoolExecutor(max_workers=parallel_jobs) as executor:
    futures = {executor.submit(process_sample, ...) for ...}
```

**Assessment:**
âœ… **Good:** Multi-process parallelism implemented  
âœ… **Good:** Progress tracking with tqdm  
âš ï¸ **Limitation:** In-memory results collection  
âš ï¸ **Limitation:** No distributed processing (single machine)

**For 10Ã— scale (1000+ samples):**
- Current: 1000 samples Ã— 7s/sample Ã· 4 workers = **~30 minutes** âœ… Acceptable
- With distributed (10 machines Ã— 4 workers): **~3 minutes** ðŸš€ Ideal

**Recommendation:** Add distributed processing layer:
```python
# Future: Celery task queue or Ray distributed computing
@ray.remote
def process_sample_distributed(sample_id, config):
    return run_pipeline(...)

futures = [process_sample_distributed.remote(s) for s in samples]
results = ray.get(futures)  # Distributed across cluster
```

### 8.4 Data Volume Scalability

**Current limits:**
- Max frames per video: ~100,000 (theoretical, tested with ~5,000)
- Max keypoints: 30 (hardcoded list in `data_loader.py`)
- Max samples per batch: Unlimited (memory-bound)

**Bottlenecks at 10Ã— data volume:**
1. **CSV parsing:** pandas `read_csv()` loads entire file into memory
   - **Solution:** Use chunked reading for >100k frames
2. **Preprocessing:** Savitzky-Golay filter on long arrays
   - **Solution:** Segment processing with overlap
3. **Visualization:** Matplotlib memory usage scales linearly
   - **Solution:** Downsampling for plots, keep full data for analysis

---

## 9. Refactoring Recommendations

### 9.1 Immediate (Priority 1 - Critical)

#### 1. Break down `cli.py::run_pipeline()` God Function

**Current:** 232-line monolithic function  
**Target:** 10-15 line orchestration function

```python
# Proposed refactoring
class GaitAnalysisPipeline:
    def __init__(self, config: GlobalConfig):
        self.config = config
        self.stages = self._build_pipeline()
    
    def _build_pipeline(self) -> List[PipelineStage]:
        return [
            DataLoadingStage(self.config),
            PreprocessingStage(self.config),
            PhaseDetectionStage(self.config),
            StepDetectionStage(self.config),
            MetricsComputationStage(self.config),
            AggregationStage(self.config),
            ExportStage(self.config)
        ]
    
    def run(self, input_files: InputFiles, output_dir: Path) -> AnalysisResult:
        context = PipelineContext(input_files, output_dir)
        
        for stage in self.stages:
            logger.info(f"Executing {stage.name}")
            context = stage.execute(context)
        
        return context.result

# cli.py becomes simple:
def run_pipeline(top, side, bottom, output, config):
    pipeline = GaitAnalysisPipeline(config)
    return pipeline.run(InputFiles(top, side, bottom), output)
```

**Benefits:**
âœ… Each stage is testable independently  
âœ… Easy to add checkpointing between stages  
âœ… Pipeline can be reordered or parallelized  
âœ… Clear separation of concerns

#### 2. Implement Configuration Schema Validation

**Current:** YAML loaded with manual checks  
**Target:** Pydantic-based schema with validation

```python
# config/models.py
from pydantic import BaseModel, Field, validator
import yaml

class GlobalConfig(BaseModel):
    fps: float = Field(120.0, ge=30, le=240)
    expected_body_length_cm: float = Field(8.0, ge=5.0, le=15.0)
    
    preprocessing: PreprocessingConfig
    phase_detection: PhaseDetectionConfig
    # ... etc
    
    @validator('fps')
    def fps_must_be_positive(cls, v):
        if v <= 0:
            raise ValueError('FPS must be positive')
        return v
    
    @classmethod
    def from_yaml(cls, path: Path) -> 'GlobalConfig':
        with open(path) as f:
            data = yaml.safe_load(f)
        return cls(**data)

# Usage:
config = GlobalConfig.from_yaml('config.yaml')
config.fps  # Type-safe access with autocomplete
```

**Benefits:**
âœ… Automatic validation with clear error messages  
âœ… IDE autocomplete for config fields  
âœ… Type safety (no more `gs.get('field', default)`)  
âœ… Self-documenting schema

#### 3. Add Basic Unit Tests

**Target:** 50% test coverage for `utils/` modules (easiest wins)

```python
# tests/test_signal_processing.py
import pytest
import numpy as np
from exmo_gait.utils.signal_processing import compute_mad, apply_savgol_filter

class TestMAD:
    def test_compute_mad_normal_distribution(self):
        data = np.random.normal(0, 1, 1000)
        mad = compute_mad(data)
        assert 0.8 < mad < 1.2  # MAD ~1 for standard normal
    
    def test_compute_mad_with_nans(self):
        data = np.array([1, 2, np.nan, 4, 5])
        mad = compute_mad(data)
        assert not np.isnan(mad)

class TestSavitzkyGolay:
    def test_smoothing_reduces_noise(self):
        signal = np.sin(np.linspace(0, 10, 100))
        noisy = signal + np.random.normal(0, 0.1, 100)
        smoothed = apply_savgol_filter(noisy, 11, 3)
        
        noise_before = np.std(noisy - signal)
        noise_after = np.std(smoothed - signal)
        assert noise_after < noise_before
```

**Testing priority:**
1. `utils/signal_processing.py` (pure functions, easy to test)
2. `utils/geometry.py` (mathematical functions)
3. `utils/validation.py` (business logic validation)
4. `core/preprocessor.py` (with mocked dependencies)
5. Integration tests for `run_pipeline()`

### 9.2 Short-term (Priority 2 - Important)

#### 4. Extract Interfaces for Key Components

```python
# interfaces/i_preprocessor.py
from abc import ABC, abstractmethod

class IPreprocessor(ABC):
    @abstractmethod
    def preprocess_trajectory(self, trajectory: np.ndarray) -> np.ndarray: ...
    
    @abstractmethod
    def compute_scale_factor(self, p1, p2, known_distance) -> float: ...

# interfaces/i_step_detector.py
class IStepDetector(ABC):
    @abstractmethod
    def detect_all_limbs(self, trajectories, windows) -> Dict: ...

# Then: dependency injection
class GaitAnalysisPipeline:
    def __init__(self, 
                 preprocessor: IPreprocessor,
                 step_detector: IStepDetector,
                 metrics_computer: IMetricsComputer):
        self.preprocessor = preprocessor
        self.step_detector = step_detector
        self.metrics_computer = metrics_computer
```

**Benefits:**
âœ… Enables mocking for tests  
âœ… Allows plugin-based extensions  
âœ… Clearer contracts between components

#### 5. Consolidate Visualizers

**Current:** `visualizer.py` + `visualizer_enhanced.py` (1049 LOC total)  
**Problem:** Code duplication, unclear when to use which

```python
# Proposed: Single visualizer with style configuration
class DashboardVisualizer:
    def __init__(self, style: VisualizerStyle = VisualizerStyle.STANDARD):
        self.style = style
    
    def generate_all_dashboards(self, ...):
        # Unified plotting logic
        if self.style == VisualizerStyle.ENHANCED:
            self._apply_enhanced_formatting()
        
        # Common plotting code (DRY)
```

#### 6. Version Management Strategy

**Current:** `_v2` methods coexist with `_v1` (unclear deprecation)

**Proposed:**
```python
# versioning.py
class VersionManager:
    CURRENT_VERSION = "1.2.0"
    
    @staticmethod
    def deprecate(version: str):
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                warnings.warn(
                    f"{func.__name__} is deprecated since v{version}",
                    DeprecationWarning,
                    stacklevel=2
                )
                return func(*args, **kwargs)
            return wrapper
        return decorator

# Usage:
class DataPreprocessor:
    @VersionManager.deprecate("1.2.0")
    def compute_scale_factor_v1(self, ...):
        # Old implementation
    
    def compute_scale_factor(self, ...):
        # Current implementation (v1.2)
```

### 9.3 Long-term (Priority 3 - Strategic)

#### 7. Plugin Architecture for Metrics

```python
# plugins/base.py
class MetricPlugin(ABC):
    @property
    @abstractmethod
    def name(self) -> str: ...
    
    @property
    @abstractmethod
    def version(self) -> str: ...
    
    @abstractmethod
    def compute(self, data: AnalysisData) -> MetricResult: ...
    
    @abstractmethod
    def visualize(self, result: MetricResult) -> Figure: ...

# plugins/loader.py
class PluginManager:
    def __init__(self, plugin_dir: Path):
        self.plugins = self._discover_plugins(plugin_dir)
    
    def _discover_plugins(self, directory: Path):
        # Load all .py files in plugins/ directory
        # Instantiate classes inheriting from MetricPlugin
        
# Usage:
manager = PluginManager(Path('plugins'))
manager.register_plugin(CustomSymmetryMetric())
results = manager.compute_all(data)
```

#### 8. Event-Driven Progress Tracking

```python
# events.py
from enum import Enum
from typing import Callable

class PipelineEvent(Enum):
    STAGE_STARTED = "stage_started"
    STAGE_COMPLETED = "stage_completed"
    PROGRESS_UPDATE = "progress_update"
    ERROR_OCCURRED = "error_occurred"

class EventBus:
    def __init__(self):
        self._listeners = {}
    
    def subscribe(self, event: PipelineEvent, callback: Callable):
        self._listeners.setdefault(event, []).append(callback)
    
    def emit(self, event: PipelineEvent, data: dict):
        for callback in self._listeners.get(event, []):
            callback(data)

# Usage:
bus = EventBus()
bus.subscribe(PipelineEvent.PROGRESS_UPDATE, lambda d: print(f"{d['stage']}: {d['percent']}%"))

class PreprocessingStage:
    def execute(self, context):
        self.bus.emit(PipelineEvent.STAGE_STARTED, {'stage': 'preprocessing'})
        # ... processing ...
        self.bus.emit(PipelineEvent.PROGRESS_UPDATE, {'stage': 'preprocessing', 'percent': 50})
        # ... more processing ...
        self.bus.emit(PipelineEvent.STAGE_COMPLETED, {'stage': 'preprocessing'})
```

#### 9. Distributed Processing Layer

```python
# distributed/worker.py
import ray

@ray.remote
class AnalysisWorker:
    def __init__(self, config: GlobalConfig):
        self.config = config
    
    def process_sample(self, sample_id: str, input_files: dict) -> dict:
        pipeline = GaitAnalysisPipeline(self.config)
        return pipeline.run(**input_files)

# distributed/coordinator.py
class DistributedCoordinator:
    def __init__(self, num_workers: int = 4):
        ray.init()
        self.workers = [AnalysisWorker.remote(config) for _ in range(num_workers)]
    
    def process_batch(self, samples: List[SampleInfo]) -> List[AnalysisResult]:
        futures = [
            worker.process_sample.remote(sample.id, sample.files)
            for worker, sample in zip(self.workers, samples)
        ]
        return ray.get(futures)
```

---

## 10. Proposed v2.0 Architecture

### 10.1 High-Level v2.0 Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      APPLICATION LAYER                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CLI (Thin Orchestrator)          â”‚  REST API (Future)            â”‚
â”‚  - Argument parsing               â”‚  - FastAPI endpoints          â”‚
â”‚  - Config loading                 â”‚  - Async job queue            â”‚
â”‚  - Pipeline invocation            â”‚  - WebSocket progress         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       DOMAIN LAYER                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Pipeline Orchestrator                                             â”‚
â”‚  â”œâ”€ Stage Registry (Factory)                                       â”‚
â”‚  â”œâ”€ Event Bus (Observer)                                          â”‚
â”‚  â”œâ”€ Checkpoint Manager                                            â”‚
â”‚  â””â”€ Error Recovery                                                â”‚
â”‚                                                                    â”‚
â”‚  Core Entities                                                     â”‚
â”‚  â”œâ”€ KeypointData (Value Object)                                   â”‚
â”‚  â”œâ”€ AnalysisResult (Aggregate Root)                               â”‚
â”‚  â”œâ”€ MetricCollection (Entity)                                     â”‚
â”‚  â””â”€ ExperimentGroup (Aggregate)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PIPELINE STAGES (Interfaces)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  IDataLoader          IPreprocessor       IPhaseDetector          â”‚
â”‚  â”œâ”€ DLCDataLoader     â”œâ”€ SavgolPreproc    â”œâ”€ MADPhaseDetector    â”‚
â”‚  â”œâ”€ SLEAPDataLoader   â””â”€ MedianPreproc    â””â”€ AdaptiveDetector    â”‚
â”‚  â””â”€ OpenPoseLoader                                                â”‚
â”‚                                                                    â”‚
â”‚  IStepDetector        IMetricsComputer    IExporter               â”‚
â”‚  â”œâ”€ VerticalDetector  â”œâ”€ GaitMetrics      â”œâ”€ XLSXExporter        â”‚
â”‚  â”œâ”€ VelocityDetector  â”œâ”€ ROMMetrics       â”œâ”€ CSVExporter         â”‚
â”‚  â””â”€ HybridDetector    â””â”€ CustomMetrics    â””â”€ JSONExporter        â”‚
â”‚                                                                    â”‚
â”‚  Plugin System: Dynamically load custom implementations           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   INFRASTRUCTURE LAYER                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Configuration         â”‚  Persistence        â”‚  Distributed       â”‚
â”‚  - Pydantic schemas    â”‚  - Result store     â”‚  - Ray cluster     â”‚
â”‚  - Env overrides       â”‚  - Cache layer      â”‚  - Task queue      â”‚
â”‚  - Migration system    â”‚  - File adapters    â”‚  - Worker pool     â”‚
â”‚                        â”‚                     â”‚                    â”‚
â”‚  Observability         â”‚  Utilities          â”‚  Validation        â”‚
â”‚  - Structured logging  â”‚  - Signal proc      â”‚  - Schema checks   â”‚
â”‚  - Metrics collector   â”‚  - Geometry math    â”‚  - Data quality    â”‚
â”‚  - Tracing (OpenTel)   â”‚  - Pure functions   â”‚  - Business rules  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 10.2 Layer Responsibilities

#### Application Layer
- **Responsibility:** User interaction, protocol translation
- **Components:** CLI, REST API, gRPC service
- **Dependencies:** Domain layer only (no direct infrastructure access)

#### Domain Layer
- **Responsibility:** Business logic, orchestration, entities
- **Components:** Pipeline orchestrator, core entities, stage interfaces
- **Dependencies:** No external dependencies (pure domain logic)

#### Pipeline Stages (Service Layer)
- **Responsibility:** Implementation of analysis algorithms
- **Components:** Concrete implementations of interfaces
- **Dependencies:** Domain interfaces + infrastructure utilities

#### Infrastructure Layer
- **Responsibility:** Technical capabilities, external integrations
- **Components:** Config, persistence, distributed computing, logging
- **Dependencies:** External libraries (Pydantic, Ray, etc.)

### 10.3 Dependency Flow (Dependency Inversion Principle)

```
Application â†’ Domain â† Pipeline Stages â† Infrastructure
     â†“          â†‘            â†“                â†“
  (uses)   (defines      (implements)     (provides)
            interfaces)
```

**Key insight:** Domain layer defines interfaces, infrastructure implements them. This allows:
- âœ… Testing domain logic without infrastructure
- âœ… Swapping implementations without changing domain
- âœ… Clear architectural boundaries

### 10.4 Example: v2.0 Pipeline Execution

```python
# main.py (Application Layer)
from domain.pipeline import PipelineBuilder, PipelineConfig
from infrastructure.config import load_config

def main():
    # Load configuration
    config = load_config('config.yaml')
    
    # Build pipeline with dependency injection
    pipeline = PipelineBuilder(config) \
        .with_data_loader('dlc') \
        .with_preprocessor('savgol') \
        .with_phase_detector('mad') \
        .with_step_detector('vertical') \
        .with_metrics(['gait', 'rom', 'custom_symmetry']) \
        .with_exporter('xlsx') \
        .with_observers([ConsoleProgress(), FileLogger()]) \
        .build()
    
    # Execute
    result = pipeline.run(
        input_files=InputFiles(top, side, bottom),
        output_dir=output_dir
    )
    
    print(result.summary())

# domain/pipeline.py (Domain Layer)
class Pipeline:
    def __init__(self, stages: List[IPipelineStage], event_bus: EventBus):
        self.stages = stages
        self.event_bus = event_bus
    
    def run(self, input_files, output_dir) -> AnalysisResult:
        context = PipelineContext(input_files, output_dir)
        
        for stage in self.stages:
            self.event_bus.emit(StageStarted(stage.name))
            
            try:
                context = stage.execute(context)
                self.event_bus.emit(StageCompleted(stage.name, context))
            except StageError as e:
                self.event_bus.emit(StageFailed(stage.name, e))
                raise PipelineError(f"Stage {stage.name} failed") from e
        
        return context.build_result()

# infrastructure/stages/data_loading.py (Infrastructure Layer)
class DLCDataLoaderStage(IPipelineStage):
    def __init__(self, config: DataLoadingConfig):
        self.config = config
    
    def execute(self, context: PipelineContext) -> PipelineContext:
        loader = MultiViewDataLoader(self.config.fps)
        loader.load_all_views(context.input_files.top, ...)
        
        context.keypoint_data = loader.get_all_keypoints()
        return context
```

### 10.5 v2.0 Key Improvements

| Aspect | v1.0 (Current) | v2.0 (Proposed) |
|--------|---------------|-----------------|
| **Testability** | 0% coverage, untestable God function | 80%+ coverage, modular stages |
| **Extensibility** | Edit 4+ files for new metric | Plugin system, register new metric |
| **Configuration** | YAML with manual validation | Pydantic schema, type-safe |
| **Error Handling** | Catch-all exception | Stage-specific errors, recovery |
| **Observability** | File logs only | Structured logging, metrics, tracing |
| **Parallelization** | Batch-level (ProcessPoolExecutor) | Stage-level + distributed (Ray) |
| **Plugin System** | None | Dynamic loading of custom components |
| **Interface Abstraction** | None | Interfaces for all major components |
| **Dependency Injection** | Hardcoded dependencies | Constructor injection |
| **Architecture Pattern** | Monolithic procedural | Layered + DDD + Hexagonal |

---

## 11. Migration Path (v1.0 â†’ v2.0)

### 11.1 Phase 1: Foundation (Weeks 1-2)

**Goal:** Establish architectural foundation without breaking existing functionality

1. **Add Pydantic configuration schema**
   - Create `config/models.py` with schema definitions
   - Migrate `config.yaml` to validated format
   - Add backward compatibility layer

2. **Extract pipeline stage interfaces**
   - Create `interfaces/` package
   - Define `IPipelineStage`, `IDataLoader`, `IPreprocessor`, etc.
   - No implementation changes yet (just type hints)

3. **Set up testing infrastructure**
   - Add `pytest`, `pytest-cov`, `pytest-mock`
   - Write first 10 unit tests for `utils/` modules
   - Set up CI pipeline (GitHub Actions)

**Validation:** All existing tests pass, config still loads

### 11.2 Phase 2: Refactor Core (Weeks 3-5)

**Goal:** Break down God function, implement stage pattern

4. **Implement pipeline orchestrator**
   - Create `PipelineOrchestrator` class
   - Extract each section of `run_pipeline()` into separate stage classes
   - Add checkpointing between stages

5. **Implement event bus for progress tracking**
   - Create `EventBus` class
   - Add progress callbacks to each stage
   - Wire up to `batch_process.py` progress bar

6. **Consolidate visualizers**
   - Merge `visualizer.py` + `visualizer_enhanced.py`
   - Extract style configuration
   - Remove code duplication

**Validation:** Full regression test suite (50+ tests)

### 11.3 Phase 3: Plugin System (Weeks 6-7)

**Goal:** Enable extensibility through plugins

7. **Implement metric plugin system**
   - Create `PluginManager` class
   - Convert existing metrics to plugin format
   - Add plugin discovery mechanism

8. **Implement data loader plugins**
   - Extract DLC loader to plugin
   - Add SLEAP loader as example
   - Document plugin API

**Validation:** Plugins load dynamically, existing metrics work

### 11.4 Phase 4: Production Hardening (Weeks 8-10)

**Goal:** Production-ready system with observability

9. **Add structured logging and metrics**
   - Integrate `structlog` for JSON logs
   - Add Prometheus metrics export
   - Add OpenTelemetry tracing

10. **Add distributed processing**
    - Integrate Ray for distributed execution
    - Add job queue for async processing
    - Add resource management

11. **Performance optimization**
    - Profile pipeline with `cProfile`
    - Optimize bottlenecks (Matplotlib, pandas)
    - Add caching layer

**Validation:** Performance benchmarks, load testing

### 11.5 Phase 5: API & Documentation (Weeks 11-12)

**Goal:** External API and comprehensive docs

12. **Add REST API**
    - FastAPI service with async endpoints
    - Job submission and status tracking
    - WebSocket for real-time progress

13. **Comprehensive documentation**
    - Architecture decision records (ADRs)
    - API documentation (OpenAPI/Swagger)
    - Plugin development guide
    - Migration guide for v1.0 users

**Validation:** API integration tests, documentation review

---

## 12. Design Decision Records (ADRs)

### ADR-001: Layered Architecture with Domain-Driven Design

**Status:** Proposed  
**Context:** Current monolithic architecture hinders extensibility  
**Decision:** Adopt layered architecture (Application â†’ Domain â†’ Infrastructure)  
**Consequences:**
- âœ… Clear separation of concerns
- âœ… Testable business logic
- âš ï¸ Increased complexity initially
- âš ï¸ Requires team training on DDD

### ADR-002: Plugin System for Metrics and Loaders

**Status:** Proposed  
**Context:** Adding new metrics requires editing multiple files  
**Decision:** Implement plugin architecture with dynamic loading  
**Consequences:**
- âœ… Easy to add custom metrics
- âœ… Third-party contributions possible
- âš ï¸ Plugin versioning complexity
- âš ï¸ Security considerations for plugin loading

### ADR-003: Pydantic for Configuration Management

**Status:** Proposed  
**Context:** YAML config lacks validation, type safety  
**Decision:** Use Pydantic for schema definition and validation  
**Consequences:**
- âœ… Type safety with IDE support
- âœ… Automatic validation
- âœ… Self-documenting schemas
- âš ï¸ Slightly verbose schema definitions

### ADR-004: Ray for Distributed Processing

**Status:** Proposed  
**Context:** Single-machine parallelism insufficient for large studies  
**Decision:** Integrate Ray for distributed computing  
**Consequences:**
- âœ… Horizontal scaling across clusters
- âœ… Unified API for local and distributed
- âš ï¸ Additional dependency (Ray runtime)
- âš ï¸ Deployment complexity increases

### ADR-005: Event-Driven Progress Tracking

**Status:** Proposed  
**Context:** No real-time feedback during long-running operations  
**Decision:** Implement event bus with observer pattern  
**Consequences:**
- âœ… Real-time progress updates
- âœ… Decoupled progress reporting
- âœ… Extensible (multiple observers)
- âš ï¸ Slight performance overhead

---

## 13. Recommendations Summary

### Immediate Actions (Do This Week)

1. âœ… **Add Pydantic config schema** - 1 day effort, huge type safety gain
2. âœ… **Write first 10 unit tests** - Start test culture, identify brittle code
3. âœ… **Extract `run_pipeline()` into stage classes** - Foundation for modularity

### Short-term Goals (This Quarter)

4. âœ… **Implement pipeline orchestrator** - Enable checkpointing, testing
5. âœ… **Consolidate visualizers** - Reduce code duplication
6. âœ… **Add event bus for progress** - Better UX for batch processing

### Long-term Vision (Next 6 Months)

7. âœ… **Build plugin system** - Enable community contributions
8. âœ… **Add distributed processing** - Scale to 1000+ samples
9. âœ… **Create REST API** - Enable web/mobile frontends

### Anti-recommendations (Do NOT Do)

âŒ **Rewrite everything from scratch** - Incremental refactoring is safer  
âŒ **Add features before tests** - Technical debt will compound  
âŒ **Optimize before profiling** - Premature optimization wastes time  
âŒ **Skip documentation** - Future maintainers will struggle

---

## 14. Conclusion

**Overall Architecture Quality:** â­â­â­â˜†â˜† (3/5)

**Strengths:**
- âœ… Clear module organization (core, analysis, export, utils)
- âœ… Comprehensive domain coverage (gait + ROM + statistics)
- âœ… Batch processing with parallelism
- âœ… Configuration-driven pipeline
- âœ… Pure utility functions (testable)

**Weaknesses:**
- âŒ Monolithic God function (`run_pipeline()`)
- âŒ Zero test coverage
- âŒ No interface abstractions
- âŒ Version fragmentation (v1.1 vs v1.2)
- âŒ Limited extensibility (hardcoded pipeline)

**Critical Path to Production:**
1. Add tests (0% â†’ 80% coverage)
2. Break down God function (monolith â†’ stages)
3. Add schema validation (YAML â†’ Pydantic)
4. Implement plugin system (rigid â†’ extensible)
5. Add observability (logs â†’ structured logs + metrics)

**Estimated Effort:** 12 weeks (1 architect + 1 developer)

**Risk Assessment:**
- **Low risk:** Utility functions, config schema, tests
- **Medium risk:** Pipeline refactoring, plugin system
- **High risk:** Distributed processing, API layer

**Recommendation:** âœ… **Proceed with incremental refactoring** following the 5-phase migration plan. Current architecture is solid for research, but requires systematic improvement for production deployment.

---

**Document Version:** 1.0  
**Author:** System Architect AI  
**Date:** 2025-11-21  
**Review Status:** Draft for stakeholder review
